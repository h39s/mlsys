{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from functools import cache\n",
    "from dataclasses import dataclass\n",
    "import typing as tp\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from transformers.models.mixtral import MixtralForCausalLM, MixtralConfig\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "from torch import nn\n",
    "from tqdm.auto import trange\n",
    "\n",
    "from hqq.core.quantize import BaseQuantizeConfig\n",
    "\n",
    "from src.expert_cache import ExpertCache\n",
    "from src.expert_wrapper import MixtralExpertWrapper\n",
    "from src.custom_layers import (\n",
    "    HQQLinearTritonSavable,\n",
    "    MixtralBLockSparseTop2MLP_HQQ,\n",
    "    SparseMoeWrapper,\n",
    ")\n",
    "from src.utils import with_default_dtype\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"mixtral-offloading\")\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from hqq.core.quantize import BaseQuantizeConfig\n",
    "from huggingface_hub import snapshot_download\n",
    "from IPython.display import clear_output\n",
    "from tqdm.auto import trange\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers.utils import logging as hf_logging\n",
    "\n",
    "from src.build_model import OffloadConfig, QuantConfig, build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
    "state_path = \"Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(quantized_model_name)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "offload_per_layer = 4\n",
    "num_experts = config.num_local_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_config = BaseQuantizeConfig(\n",
    "    nbits=4,\n",
    "    group_size=64,\n",
    "    quant_zero=True,\n",
    "    quant_scale=True,\n",
    ")\n",
    "attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n",
    "\n",
    "\n",
    "ffn_config = BaseQuantizeConfig(\n",
    "    nbits=2,\n",
    "    group_size=16,\n",
    "    quant_zero=True,\n",
    "    quant_scale=True,\n",
    ")\n",
    "quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta1, meta2 = quant_config.get_ffn_metas(\n",
    "    config.hidden_size, config.intermediate_size\n",
    ")\n",
    "mblock = MixtralBLockSparseTop2MLP_HQQ(config, quant_config.ffn_config, meta1, meta2)\n",
    "mblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "hqqlayer = HQQLinearTritonSavable(nn.Linear(128, 128, bias=False), quant_config.ffn_config, use_gpu=False)\n",
    "inp = torch.randn(1, 128)\n",
    "out = hqqlayer(inp)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqqlayer(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
